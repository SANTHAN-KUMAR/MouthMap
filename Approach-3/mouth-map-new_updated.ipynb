{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"mount_file_id":"1PIAmKRaxfc_uR8wuOVK9LGmhyFZzzY9F","authorship_tag":"ABX9TyPifF2gEeCA24CqwBm5P3yz"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10586871,"sourceType":"datasetVersion","datasetId":6551962}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\n\n# Redirect stdout to /dev/null\noriginal_stdout = sys.stdout\nsys.stdout = open(os.devnull, 'w')\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Restore stdout\nsys.stdout = original_stdout\n\n# print the acknowledgement\nprint(\"Finished processing files.\")","metadata":{"id":"dg89R1HROj_h","executionInfo":{"status":"ok","timestamp":1736345049736,"user_tz":-330,"elapsed":546,"user":{"displayName":"Senthan Karnala","userId":"01475679638638809375"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:41:19.131816Z","iopub.execute_input":"2025-02-19T08:41:19.132122Z","iopub.status.idle":"2025-02-19T08:41:21.863968Z","shell.execute_reply.started":"2025-02-19T08:41:19.132091Z","shell.execute_reply":"2025-02-19T08:41:21.863221Z"}},"outputs":[{"name":"stdout","text":"Finished processing files.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, BatchNormalization, Reshape\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom glob import glob\nimport cv2\n\nclass LipReadingDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, data_path, alignment_path, batch_size=16, frame_length=75, \n                 image_height=46, image_width=140, video_paths=None, alignment_paths=None, \n                 fixed_vocabulary=None, **kwargs):\n        super().__init__(**kwargs)\n        self.data_path = data_path\n        self.alignment_path = alignment_path\n        self.batch_size = batch_size\n        self.frame_length = frame_length\n        self.image_height = image_height\n        self.image_width = image_width\n\n        # Use provided paths or generate from directory\n        if video_paths is None:\n            self.video_paths = sorted(glob(os.path.join(data_path, '*.mpg')))\n        else:\n            self.video_paths = video_paths\n        \n        if alignment_paths is None:\n            self.alignment_paths = sorted(glob(os.path.join(alignment_path, '*.align')))\n        else:\n            self.alignment_paths = alignment_paths\n        \n        print(f\"Found {len(self.video_paths)} video files and {len(self.alignment_paths)} alignment files\")\n        \n        # Use fixed vocabulary if provided, otherwise create from data\n        if fixed_vocabulary is not None:\n            self.vocabulary = fixed_vocabulary\n        else:\n            self.vocabulary = self._create_word_vocabulary()\n            \n        self.char_to_num = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\")\n        self.num_to_char = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\", invert=True)\n\n    def _create_word_vocabulary(self):\n        words = set()\n        print(f\"Processing alignment files from: {self.alignment_path}\")\n        \n        for align_path in self.alignment_paths:\n            try:\n                with open(align_path, 'r') as f:\n                    content = f.read().strip().split()\n                    words.update([content[i] for i in range(2, len(content), 3)])\n            except Exception as e:\n                print(f\"Error processing {align_path}: {str(e)}\")\n        \n        words.discard('sil')\n        vocabulary = sorted(list(words))\n        \n        if not vocabulary:\n            print(\"No words found in alignment files. Using default vocabulary.\")\n            vocabulary = ['bin', 'blue', 'at', 'f', 'two', 'now']\n        \n        print(f\"Vocabulary size: {len(vocabulary)}\")\n        return vocabulary\n\n    def __len__(self):\n        return max(1, len(self.video_paths) // self.batch_size)\n    \n    def _process_video(self, video_path):\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            mouth = gray[190:236, 80:220]\n            mouth = cv2.resize(mouth, (self.image_width, self.image_height))\n            frames.append(mouth)\n        \n        cap.release()\n        \n        frames = np.array(frames, dtype=np.float32)\n        frames = (frames - frames.mean()) / (frames.std() + 1e-6)\n        \n        if len(frames) < self.frame_length:\n            pad_length = self.frame_length - len(frames)\n            frames = np.pad(frames, ((0, pad_length), (0, 0), (0, 0)), mode='constant')\n        else:\n            frames = frames[:self.frame_length]\n        \n        return frames\n    \n    def _process_alignment(self, alignment_path):\n        with open(alignment_path, 'r') as f:\n            content = f.read().strip().split()\n        \n        words = [content[i] for i in range(2, len(content), 3) if content[i] != 'sil']\n        text = ' '.join(words)\n        return self.char_to_num(tf.convert_to_tensor(text.split()))\n    \n    def __getitem__(self, idx):\n        batch_videos = self.video_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_alignments = self.alignment_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        X = np.zeros((len(batch_videos), self.frame_length, self.image_height, self.image_width, 1))\n        Y = np.zeros((len(batch_videos), len(self.vocabulary)))\n        \n        for i, (video_path, align_path) in enumerate(zip(batch_videos, batch_alignments)):\n            frames = self._process_video(video_path)\n            X[i] = frames.reshape(self.frame_length, self.image_height, self.image_width, 1)\n            \n            labels = self._process_alignment(align_path)\n            Y[i] = tf.reduce_max(tf.one_hot(labels, len(self.vocabulary)), axis=0)\n        \n        return X, Y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:41:21.865069Z","iopub.execute_input":"2025-02-19T08:41:21.865411Z","iopub.status.idle":"2025-02-19T08:41:21.882819Z","shell.execute_reply.started":"2025-02-19T08:41:21.865379Z","shell.execute_reply":"2025-02-19T08:41:21.882143Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def build_model(frame_length, image_height, image_width, vocabulary_size):\n    model = Sequential([\n        tf.keras.Input(shape=(frame_length, image_height, image_width, 1)), \n        Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        Conv3D(128, kernel_size=(3, 3, 3), activation='relu'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        Conv3D(256, kernel_size=(3, 3, 3), activation='relu'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        Reshape((-1, 256)),\n        \n        Bidirectional(LSTM(128, return_sequences=True)),\n        Dropout(0.5),\n        \n        Bidirectional(LSTM(64)),\n        Dropout(0.5),\n        \n        Dense(256, activation='relu'),\n        Dropout(0.5),\n        Dense(vocabulary_size, activation='softmax')\n\n    ])\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:41:21.884356Z","iopub.execute_input":"2025-02-19T08:41:21.884635Z","iopub.status.idle":"2025-02-19T08:41:21.896655Z","shell.execute_reply.started":"2025-02-19T08:41:21.884612Z","shell.execute_reply":"2025-02-19T08:41:21.895872Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# TRAINING THE MODEL ON FIRST HALF SET OF THE DATA ALONE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:41:21.897703Z","iopub.execute_input":"2025-02-19T08:41:21.897961Z","iopub.status.idle":"2025-02-19T08:41:21.908059Z","shell.execute_reply.started":"2025-02-19T08:41:21.897936Z","shell.execute_reply":"2025-02-19T08:41:21.907397Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import os\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau  # Import ReduceLROnPlateau\n\ndef create_callbacks(model_dir, model_name, monitor='val_loss', patience=5):\n    \"\"\"Creates and returns a list of Keras callbacks.\"\"\"\n\n    checkpoint_path = os.path.join(model_dir, f\"{model_name}_best.keras\")  # Saves best model\n    checkpoint = ModelCheckpoint(\n        checkpoint_path,\n        monitor=monitor,\n        save_best_only=True,\n        save_weights_only=False,\n        verbose=1\n    )\n\n    early_stopping = EarlyStopping(\n        monitor=monitor,\n        patience=patience,\n        restore_best_weights=True,  # Important: Restores the best weights\n        verbose=1\n    )\n\n    reduce_lr = ReduceLROnPlateau(\n        monitor=monitor,\n        factor=0.5,  # Reduce learning rate by half\n        patience=patience // 2,  # Reduce LR patience usually less than ES\n        min_lr=1e-6,  # Set a minimum learning rate\n        verbose=1\n    )\n\n    return [checkpoint, early_stopping, reduce_lr]\n\ndef train_and_save_main_model(data_dir, alignment_dir, batch_size=16):\n    print(\"Starting training for sentence-level prediction...\")\n\n    model_dir = \"models_main\"\n    os.makedirs(model_dir, exist_ok=True)\n\n    full_data_generator = LipReadingDataGenerator(data_dir, alignment_dir, batch_size=batch_size)\n    full_vocabulary = full_data_generator.vocabulary\n\n    total_videos = len(full_data_generator.video_paths)\n    mid_point = total_videos // 2\n\n    # --- First Half Training ---\n    first_half_generator = LipReadingDataGenerator(\n        data_path=data_dir,\n        alignment_path=alignment_dir,\n        batch_size=batch_size,\n        video_paths=full_data_generator.video_paths[:mid_point],\n        alignment_paths=full_data_generator.alignment_paths[:mid_point],\n        fixed_vocabulary=full_vocabulary\n    )\n\n    model = build_model(frame_length=75, image_height=46, image_width=140, vocabulary_size=len(full_vocabulary))\n    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n    first_half_callbacks = create_callbacks(model_dir, 'lip_reading_first_half', monitor='accuracy')\n    train_model(model, first_half_generator, epochs=30, callbacks=first_half_callbacks, dataset_split_name=\"first half\")\n\n\n    # --- Save the model and vocabulary after training on the first half ---\n    save_model_and_vocabulary(model, model_dir, 'lip_reading_first_half', full_vocabulary)\n\n    print(\"Training on the first half complete.  Model and vocabulary saved. \"\n          \"You can now evaluate the model's performance on the first half \"\n          \"and decide whether to proceed with training on the second half.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:41:21.908787Z","iopub.execute_input":"2025-02-19T08:41:21.908987Z","iopub.status.idle":"2025-02-19T08:41:21.920735Z","shell.execute_reply.started":"2025-02-19T08:41:21.908960Z","shell.execute_reply":"2025-02-19T08:41:21.920005Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ACTUAL CODE OF FULL TRAINED MODEL ( UPDATED ON GITHUB )\n\"\"\"def train_and_save_main_model(data_dir, alignment_dir, batch_size=16):\n    print(\"Starting training for sentence-level prediction...\")\n    \n    model_dir = \"models_main\"\n    os.makedirs(model_dir, exist_ok=True)\n    \n    # Create full data generator to get consistent vocabulary\n    full_data_generator = LipReadingDataGenerator(data_dir, alignment_dir, batch_size=batch_size)\n    full_vocabulary = full_data_generator.vocabulary\n    \n    # Split video paths into two halves\n    total_videos = len(full_data_generator.video_paths)\n    mid_point = total_videos // 2\n    \n    # First half training\n    first_half_generator = LipReadingDataGenerator(\n        data_path=data_dir, \n        alignment_path=alignment_dir, \n        batch_size=batch_size,\n        video_paths=full_data_generator.video_paths[:mid_point],\n        alignment_paths=full_data_generator.alignment_paths[:mid_point],\n        fixed_vocabulary=full_vocabulary\n    )\n    \n    # Build and compile the model\n    model = build_model(\n        frame_length=75,\n        image_height=46,\n        image_width=140,\n        vocabulary_size=len(full_vocabulary)\n    )\n    \n    model.compile(\n        optimizer=Adam(learning_rate=0.0001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Callbacks for first half\n    first_half_callbacks = [\n        ModelCheckpoint(\n            os.path.join(model_dir, 'lip_reading_first_half_best.keras'),\n            save_best_only=True,\n            monitor='accuracy'\n        ),\n        EarlyStopping(\n            monitor='loss',\n            patience=7,\n            restore_best_weights=True\n        )\n    ]\n\n    # Train on first half\n    print(\"Training on first half of the dataset...\")\n    model.fit(\n        first_half_generator,\n        epochs=30,\n        callbacks=first_half_callbacks\n    )\n    \n    # Second half training\n    second_half_generator = LipReadingDataGenerator(\n        data_path=data_dir, \n        alignment_path=alignment_dir, \n        batch_size=batch_size,\n        video_paths=full_data_generator.video_paths[mid_point:],\n        alignment_paths=full_data_generator.alignment_paths[mid_point:],\n        fixed_vocabulary=full_vocabulary\n    )\n    \n    # Callbacks for second half\n    second_half_callbacks = [\n        ModelCheckpoint(\n            os.path.join(model_dir, 'lip_reading_full_dataset_best.keras'),\n            save_best_only=True,\n            monitor='accuracy'\n        ),\n        EarlyStopping(\n            monitor='loss',\n            patience=7,\n            restore_best_weights=True\n        )\n    ]\n\n    # Train on second half\n    print(\"Training on second half of the dataset...\")\n    model.fit(\n        second_half_generator,\n        epochs=30,\n        callbacks=second_half_callbacks\n    )\n\n    # Save final model\n    final_model_path = os.path.join(model_dir, 'lip_reading_full_model.h5')\n    model.save(final_model_path)\n    print(f\"Final model saved: {final_model_path}\")\n\n    # Save vocabulary\n    vocab_path = os.path.join(model_dir, 'lip_reading_full_model.h5')\n    model.save(final_model_path)\n    print(f\"Final model saved: {final_model_path}\")\n\n    # Save vocabulary\n    vocab_path = os.path.join(model_dir, 'vocabulary_main.txt')\n    with open(vocab_path, 'w') as f:\n        f.write('\\n'.join(full_vocabulary))\n    print(f\"Vocabulary saved: {vocab_path}\")\n\"\"\"\n;","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:41:21.921451Z","iopub.execute_input":"2025-02-19T08:41:21.921621Z","iopub.status.idle":"2025-02-19T08:41:21.935168Z","shell.execute_reply.started":"2025-02-19T08:41:21.921606Z","shell.execute_reply":"2025-02-19T08:41:21.934306Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"''"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"\"\"\"if __name__ == \"__main__\":\n    data_dir = r\"/kaggle/input/mouth-map-comp/data/s1\"\n    alignment_dir = r\"/kaggle/input/mouth-map-comp/data/alignments/s1\"\n    \n    print(\"Training main model...\")\n    train_and_save_main_model(data_dir, alignment_dir)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:41:21.936041Z","iopub.execute_input":"2025-02-19T08:41:21.936278Z","iopub.status.idle":"2025-02-19T08:41:21.948105Z","shell.execute_reply.started":"2025-02-19T08:41:21.936260Z","shell.execute_reply":"2025-02-19T08:41:21.947444Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'if __name__ == \"__main__\":\\n    data_dir = r\"/kaggle/input/mouth-map-comp/data/s1\"\\n    alignment_dir = r\"/kaggle/input/mouth-map-comp/data/alignments/s1\"\\n    \\n    print(\"Training main model...\")\\n    train_and_save_main_model(data_dir, alignment_dir)\\n'"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"#######   GENERATED USING GPT LOGICAL REASONING.\n## DEEPSEEK, GEMINI, KIMI'S APPRAOCHES ARE YET TO BE TESTED ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, BatchNormalization, Reshape\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom glob import glob\nimport cv2\n\n# -------------------------------------------------------------------\n# Data Generator for Lip Reading\n# -------------------------------------------------------------------\nclass LipReadingDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, data_path, alignment_path, batch_size=16, frame_length=75, \n                 image_height=46, image_width=140, video_paths=None, alignment_paths=None, \n                 fixed_vocabulary=None, **kwargs):\n        super().__init__(**kwargs)\n        self.data_path = data_path\n        self.alignment_path = alignment_path\n        self.batch_size = batch_size\n        self.frame_length = frame_length\n        self.image_height = image_height\n        self.image_width = image_width\n\n        # Use provided paths or search the directory\n        if video_paths is None:\n            self.video_paths = sorted(glob(os.path.join(data_path, '*.mpg')))\n        else:\n            self.video_paths = video_paths\n        \n        if alignment_paths is None:\n            self.alignment_paths = sorted(glob(os.path.join(alignment_path, '*.align')))\n        else:\n            self.alignment_paths = alignment_paths\n        \n        print(f\"Found {len(self.video_paths)} video files and {len(self.alignment_paths)} alignment files\")\n        \n        # Build vocabulary either from the fixed vocabulary provided or from the data.\n        if fixed_vocabulary is not None:\n            self.vocabulary = fixed_vocabulary\n        else:\n            self.vocabulary = self._create_word_vocabulary()\n            \n        # Create lookup layers for converting between words and numbers.\n        self.char_to_num = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\"\n        )\n        self.num_to_char = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\", invert=True\n        )\n\n    def _create_word_vocabulary(self):\n        words = set()\n        print(f\"Processing alignment files from: {self.alignment_path}\")\n        for align_path in self.alignment_paths:\n            try:\n                with open(align_path, 'r') as f:\n                    content = f.read().strip().split()\n                    # The alignment file is assumed to have a pattern where every third token (starting at index 2) is a word.\n                    words.update([content[i] for i in range(2, len(content), 3)])\n            except Exception as e:\n                print(f\"Error processing {align_path}: {str(e)}\")\n        # Remove the silence token if present\n        words.discard('sil')\n        vocabulary = sorted(list(words))\n        if not vocabulary:\n            print(\"No words found in alignment files. Using default vocabulary.\")\n            vocabulary = ['bin', 'blue', 'at', 'f', 'two', 'now']\n        print(f\"Vocabulary size: {len(vocabulary)}\")\n        return vocabulary\n\n    def __len__(self):\n        return max(1, len(self.video_paths) // self.batch_size)\n    \n    def _process_video(self, video_path):\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            # Convert to grayscale\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            # Crop to the mouth region (hard-coded crop coordinates)\n            mouth = gray[190:236, 80:220]\n            # Resize the cropped image to the desired size\n            mouth = cv2.resize(mouth, (self.image_width, self.image_height))\n            frames.append(mouth)\n        cap.release()\n        frames = np.array(frames, dtype=np.float32)\n        # Normalize the frames: zero mean and unit variance\n        frames = (frames - frames.mean()) / (frames.std() + 1e-6)\n        # Pad if the number of frames is less than frame_length, or trim if more\n        if len(frames) < self.frame_length:\n            pad_length = self.frame_length - len(frames)\n            frames = np.pad(frames, ((0, pad_length), (0, 0), (0, 0)), mode='constant')\n        else:\n            frames = frames[:self.frame_length]\n        return frames\n    \n    def _process_alignment(self, alignment_path):\n        with open(alignment_path, 'r') as f:\n            content = f.read().strip().split()\n        # Extract every third token starting at index 2 (ignoring 'sil')\n        words = [content[i] for i in range(2, len(content), 3) if content[i] != 'sil']\n        text = ' '.join(words)\n        # Convert words to numerical IDs using the lookup layer\n        return self.char_to_num(tf.convert_to_tensor(text.split()))\n    \n    def __getitem__(self, idx):\n        batch_videos = self.video_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_alignments = self.alignment_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        # Initialize arrays for the batch data.\n        X = np.zeros((len(batch_videos), self.frame_length, self.image_height, self.image_width, 1))\n        Y = np.zeros((len(batch_videos), len(self.vocabulary)))\n        \n        for i, (video_path, align_path) in enumerate(zip(batch_videos, batch_alignments)):\n            frames = self._process_video(video_path)\n            # Add channel dimension (1 for grayscale)\n            X[i] = frames.reshape(self.frame_length, self.image_height, self.image_width, 1)\n            \n            # Process alignment and convert to a multi-hot vector.\n            labels = self._process_alignment(align_path)\n            # One-hot encode each label and then use reduce_max to combine them into a single vector.\n            Y[i] = tf.reduce_max(tf.one_hot(labels, len(self.vocabulary)), axis=0)\n        \n        return X, Y\n\n# -------------------------------------------------------------------\n# Model Architecture\n# -------------------------------------------------------------------\ndef build_model(frame_length, image_height, image_width, vocabulary_size):\n    model = Sequential([\n        tf.keras.Input(shape=(frame_length, image_height, image_width, 1)),\n        # First 3D convolution block\n        Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        # Second 3D convolution block\n        Conv3D(128, kernel_size=(3, 3, 3), activation='relu'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        # Third 3D convolution block\n        Conv3D(256, kernel_size=(3, 3, 3), activation='relu'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        # Reshape to combine the spatial dimensions into a feature vector per time step.\n        Reshape((-1, 256)),\n        \n        # Temporal modeling with Bidirectional LSTMs\n        Bidirectional(LSTM(128, return_sequences=True)),\n        Dropout(0.5),\n        Bidirectional(LSTM(64)),\n        Dropout(0.5),\n        \n        # Dense layers for classification\n        Dense(256, activation='relu'),\n        Dropout(0.5),\n        Dense(vocabulary_size, activation='softmax')\n    ])\n    \n    return model\n\n# -------------------------------------------------------------------\n# Revised Training Routine\n# -------------------------------------------------------------------\ndef train_and_save_model(data_dir, alignment_dir, batch_size=16, epochs=30):\n    # -----------------------------------------------------------\n    # Step 1: Build the full vocabulary using all available data.\n    # -----------------------------------------------------------\n    full_data_generator = LipReadingDataGenerator(data_dir, alignment_dir, batch_size=batch_size)\n    full_vocabulary = full_data_generator.vocabulary\n    \n    # Retrieve all file paths (assumes they are in the same order)\n    all_video_paths = full_data_generator.video_paths\n    all_align_paths = full_data_generator.alignment_paths\n    \n    total_files = len(all_video_paths)\n    print(f\"Total number of files: {total_files}\")\n    \n    # -----------------------------------------------------------\n    # Step 2: Split the data into training and validation sets.\n    # Training: first half of the data.\n    # Validation: 70% of the second half.\n    # -----------------------------------------------------------\n    mid_point = total_files // 2\n    train_video_paths = all_video_paths[:mid_point]\n    train_align_paths = all_align_paths[:mid_point]\n    \n    second_half_video_paths = all_video_paths[mid_point:]\n    second_half_align_paths = all_align_paths[mid_point:]\n    \n    # Use 70% of the second half for validation.\n    val_count = int(len(second_half_video_paths) * 0.7)\n    val_video_paths = second_half_video_paths[:val_count]\n    val_align_paths = second_half_align_paths[:val_count]\n    \n    print(f\"Training on {len(train_video_paths)} files.\")\n    print(f\"Validating on {len(val_video_paths)} files.\")\n    \n    # -----------------------------------------------------------\n    # Step 3: Create data generators for training and validation.\n    # -----------------------------------------------------------\n    train_generator = LipReadingDataGenerator(\n        data_path=data_dir, \n        alignment_path=alignment_dir, \n        batch_size=batch_size,\n        video_paths=train_video_paths,\n        alignment_paths=train_align_paths,\n        fixed_vocabulary=full_vocabulary\n    )\n    \n    val_generator = LipReadingDataGenerator(\n        data_path=data_dir, \n        alignment_path=alignment_dir, \n        batch_size=batch_size,\n        video_paths=val_video_paths,\n        alignment_paths=val_align_paths,\n        fixed_vocabulary=full_vocabulary\n    )\n    \n    # -----------------------------------------------------------\n    # Step 4: Build, compile, and train the model.\n    # -----------------------------------------------------------\n    model = build_model(\n        frame_length=75,\n        image_height=46,\n        image_width=140,\n        vocabulary_size=len(full_vocabulary)\n    )\n    \n    model.compile(\n        optimizer=Adam(learning_rate=0.0001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    model_dir = \"models_main\"\n    os.makedirs(model_dir, exist_ok=True)\n    \n    # Callbacks to save the best model and to perform early stopping.\n    callbacks = [\n        ModelCheckpoint(\n            os.path.join(model_dir, 'lip_reading_best_model.keras'),\n            save_best_only=True,\n            monitor='val_accuracy',\n            mode='max'\n        ),\n        EarlyStopping(\n            monitor='val_loss',\n            patience=7,\n            restore_best_weights=True\n        )\n    ]\n    \n    print(\"Starting training...\")\n    history = model.fit(\n        train_generator,\n        epochs=epochs,\n        validation_data=val_generator,\n        callbacks=callbacks\n    )\n    \n    # -----------------------------------------------------------\n    # Step 5: Save the final model and the vocabulary.\n    # -----------------------------------------------------------\n    final_model_path = os.path.join(model_dir, 'lip_reading_full_model.h5')\n    model.save(final_model_path)\n    print(f\"Final model saved: {final_model_path}\")\n    \n    vocab_path = os.path.join(model_dir, 'vocabulary_main.txt')\n    with open(vocab_path, 'w') as f:\n        f.write('\\n'.join(full_vocabulary))\n    print(f\"Vocabulary saved: {vocab_path}\")\n    \n    return model, history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:44:39.033373Z","iopub.execute_input":"2025-02-19T08:44:39.033669Z","iopub.status.idle":"2025-02-19T08:44:39.057439Z","shell.execute_reply.started":"2025-02-19T08:44:39.033645Z","shell.execute_reply":"2025-02-19T08:44:39.056526Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"\n# -------------------------------------------------------------------\n# Example usage:\n# Set your directories for the video (.mpg) files and alignment (.align) files.\ndata_dir = r\"/kaggle/input/mouth-map-comp/data/s1\"\nalignment_dir = r\"/kaggle/input/mouth-map-comp/data/alignments/s1\"\ntrain_and_save_model(data_dir, alignment_dir, batch_size=16, epochs=30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:44:45.859710Z","iopub.execute_input":"2025-02-19T08:44:45.860002Z","iopub.status.idle":"2025-02-19T09:00:31.981864Z","shell.execute_reply.started":"2025-02-19T08:44:45.859979Z","shell.execute_reply":"2025-02-19T09:00:31.981091Z"}},"outputs":[{"name":"stdout","text":"Found 1000 video files and 1000 alignment files\nProcessing alignment files from: /kaggle/input/mouth-map-comp/data/alignments/s1\nVocabulary size: 52\nTotal number of files: 1000\nTraining on 500 files.\nValidating on 350 files.\nFound 500 video files and 500 alignment files\nFound 350 video files and 350 alignment files\nStarting training...\nEpoch 1/30\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 4s/step - accuracy: 0.0411 - loss: 23.3512 - val_accuracy: 0.0060 - val_loss: 23.3720\nEpoch 2/30\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 3s/step - accuracy: 0.0640 - loss: 22.9763 - val_accuracy: 0.0000e+00 - val_loss: 23.3792\nEpoch 3/30\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 3s/step - accuracy: 0.0721 - loss: 23.0465 - val_accuracy: 0.0000e+00 - val_loss: 23.9857\nEpoch 4/30\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 3s/step - accuracy: 0.1188 - loss: 25.1400 - val_accuracy: 0.0000e+00 - val_loss: 25.9196\nEpoch 5/30\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 3s/step - accuracy: 0.1116 - loss: 29.0962 - val_accuracy: 0.0000e+00 - val_loss: 30.4129\nEpoch 6/30\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 3s/step - accuracy: 0.1225 - loss: 33.9973 - val_accuracy: 0.0000e+00 - val_loss: 37.0418\nEpoch 7/30\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 3s/step - accuracy: 0.0711 - loss: 39.8804 - val_accuracy: 0.0000e+00 - val_loss: 42.5676\nEpoch 8/30\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 3s/step - accuracy: 0.0778 - loss: 45.9660 - val_accuracy: 0.0000e+00 - val_loss: 52.6853\nFinal model saved: models_main/lip_reading_full_model.h5\nVocabulary saved: models_main/vocabulary_main.txt\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(<Sequential name=sequential_6, built=True>,\n <keras.src.callbacks.history.History at 0x7efd4496a530>)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"def predict_on_video(model_path, video_path, vocabulary_path, frame_length=75, \n                     image_height=46, image_width=140):\n    \"\"\"Make prediction on a single video file\"\"\"\n    # Load vocabulary\n    with open(vocabulary_path, 'r') as f:\n        vocabulary = f.read().strip().split('\\n')\n    \n    # Load model\n    model = load_model(model_path)\n    \n    # Process video\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        mouth = gray[190:236, 80:220]\n        mouth = cv2.resize(mouth, (image_width, image_height))\n        frames.append(mouth)\n    \n    cap.release()\n    \n    frames = np.array(frames, dtype=np.float32)\n    frames = (frames - frames.mean()) / (frames.std() + 1e-6)\n    \n    if len(frames) < frame_length:\n        pad_length = frame_length - len(frames)\n        frames = np.pad(frames, ((0, pad_length), (0, 0), (0, 0)), mode='constant')\n    else:\n        frames = frames[:frame_length]\n    \n    frames = frames.reshape(1, frame_length, image_height, image_width, 1)\n    \n    # Make prediction\n    prediction = model.predict(frames)[0]\n    top_indices = np.argsort(prediction)[-5:][::-1]  # Get top 5 predictions\n    \n    results = []\n    for idx in top_indices:\n        results.append({\n            'word': vocabulary[idx],\n            'confidence': float(prediction[idx])\n        })\n    \n    return results\n\nresults = predict_on_video(\n    model_path=\"/kaggle/working/models_main/lip_reading_best_model.keras\",\n    video_path=\"/kaggle/input/mouth-map-comp/data/s1/bbaf2n.mpg\",\n    vocabulary_path=\"/kaggle/working/models_main/vocabulary_main.txt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:07:51.527640Z","iopub.execute_input":"2025-02-19T09:07:51.527959Z","iopub.status.idle":"2025-02-19T09:07:52.974846Z","shell.execute_reply.started":"2025-02-19T09:07:51.527933Z","shell.execute_reply":"2025-02-19T09:07:52.974032Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874ms/step\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:08:02.976623Z","iopub.execute_input":"2025-02-19T09:08:02.976906Z","iopub.status.idle":"2025-02-19T09:08:02.982201Z","shell.execute_reply.started":"2025-02-19T09:08:02.976884Z","shell.execute_reply":"2025-02-19T09:08:02.981249Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"[{'word': 'blue', 'confidence': 0.02360173501074314},\n {'word': 'j', 'confidence': 0.022521479055285454},\n {'word': 's', 'confidence': 0.022424446418881416},\n {'word': 'm', 'confidence': 0.022119032219052315},\n {'word': 'x', 'confidence': 0.022104639559984207}]"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}