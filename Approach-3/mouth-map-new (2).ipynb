{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"mount_file_id":"1PIAmKRaxfc_uR8wuOVK9LGmhyFZzzY9F","authorship_tag":"ABX9TyPifF2gEeCA24CqwBm5P3yz"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10586871,"sourceType":"datasetVersion","datasetId":6551962}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\n\n# Redirect stdout to /dev/null\noriginal_stdout = sys.stdout\nsys.stdout = open(os.devnull, 'w')\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Restore stdout\nsys.stdout = original_stdout\n\n# print the acknowledgement\nprint(\"Finished processing files.\")","metadata":{"id":"dg89R1HROj_h","executionInfo":{"status":"ok","timestamp":1736345049736,"user_tz":-330,"elapsed":546,"user":{"displayName":"Senthan Karnala","userId":"01475679638638809375"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:58:51.046260Z","iopub.execute_input":"2025-02-21T12:58:51.046472Z","iopub.status.idle":"2025-02-21T12:58:53.772884Z","shell.execute_reply.started":"2025-02-21T12:58:51.046452Z","shell.execute_reply":"2025-02-21T12:58:53.772120Z"}},"outputs":[{"name":"stdout","text":"Finished processing files.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, BatchNormalization, Reshape\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom glob import glob\nimport cv2\n\n# -------------------------------------------------------------------\n# Data Generator for Lip Reading\n# -------------------------------------------------------------------\nclass LipReadingDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, data_path, alignment_path, batch_size=16, frame_length=75, \n                 image_height=46, image_width=140, video_paths=None, alignment_paths=None, \n                 fixed_vocabulary=None, **kwargs):\n        super().__init__(**kwargs)\n        self.data_path = data_path\n        self.alignment_path = alignment_path\n        self.batch_size = batch_size\n        self.frame_length = frame_length\n        self.image_height = image_height\n        self.image_width = image_width\n\n        # Use provided paths or search the directory\n        if video_paths is None:\n            self.video_paths = sorted(glob(os.path.join(data_path, '*.mpg')))\n        else:\n            self.video_paths = video_paths\n        \n        if alignment_paths is None:\n            self.alignment_paths = sorted(glob(os.path.join(alignment_path, '*.align')))\n        else:\n            self.alignment_paths = alignment_paths\n        \n        print(f\"Found {len(self.video_paths)} video files and {len(self.alignment_paths)} alignment files\")\n        \n        # Build vocabulary either from the fixed vocabulary provided or from the data.\n        if fixed_vocabulary is not None:\n            self.vocabulary = fixed_vocabulary\n        else:\n            self.vocabulary = self._create_word_vocabulary()\n            \n        # Create lookup layers for converting between words and numbers.\n        self.char_to_num = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\"\n        )\n        self.num_to_char = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\", invert=True\n        )\n\n    def _create_word_vocabulary(self):\n        words = set()\n        print(f\"Processing alignment files from: {self.alignment_path}\")\n        for align_path in self.alignment_paths:\n            try:\n                with open(align_path, 'r') as f:\n                    content = f.read().strip().split()\n                    # The alignment file is assumed to have a pattern where every third token (starting at index 2) is a word.\n                    words.update([content[i] for i in range(2, len(content), 3)])\n            except Exception as e:\n                print(f\"Error processing {align_path}: {str(e)}\")\n        # Remove the silence token if present\n        words.discard('sil')\n        vocabulary = sorted(list(words))\n        if not vocabulary:\n            print(\"No words found in alignment files. Using default vocabulary.\")\n            vocabulary = ['bin', 'blue', 'at', 'f', 'two', 'now']\n        print(f\"Vocabulary size: {len(vocabulary)}\")\n        return vocabulary\n\n    def __len__(self):\n        return max(1, len(self.video_paths) // self.batch_size)\n    \n    def _process_video(self, video_path):\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            # Convert to grayscale\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            # Crop to the mouth region (hard-coded crop coordinates)\n            mouth = gray[190:236, 80:220]\n            # Resize the cropped image to the desired size\n            mouth = cv2.resize(mouth, (self.image_width, self.image_height))\n            frames.append(mouth)\n        cap.release()\n        frames = np.array(frames, dtype=np.float32)\n        # Normalize the frames: zero mean and unit variance\n        frames = (frames - frames.mean()) / (frames.std() + 1e-6)\n        # Pad if the number of frames is less than frame_length, or trim if more\n        if len(frames) < self.frame_length:\n            pad_length = self.frame_length - len(frames)\n            frames = np.pad(frames, ((0, pad_length), (0, 0), (0, 0)), mode='constant')\n        else:\n            frames = frames[:self.frame_length]\n        return frames\n    \n    def _process_alignment(self, alignment_path):\n        with open(alignment_path, 'r') as f:\n            content = f.read().strip().split()\n        # Extract every third token starting at index 2 (ignoring 'sil')\n        words = [content[i] for i in range(2, len(content), 3) if content[i] != 'sil']\n        text = ' '.join(words)\n        # Convert words to numerical IDs using the lookup layer\n        return self.char_to_num(tf.convert_to_tensor(text.split()))\n    \n    def __getitem__(self, idx):\n        batch_videos = self.video_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_alignments = self.alignment_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        # Initialize arrays for the batch data.\n        X = np.zeros((len(batch_videos), self.frame_length, self.image_height, self.image_width, 1))\n        Y = np.zeros((len(batch_videos), len(self.vocabulary)))\n        \n        for i, (video_path, align_path) in enumerate(zip(batch_videos, batch_alignments)):\n            frames = self._process_video(video_path)\n            # Add channel dimension (1 for grayscale)\n            X[i] = frames.reshape(self.frame_length, self.image_height, self.image_width, 1)\n            \n            # Process alignment and convert to a multi-hot vector.\n            labels = self._process_alignment(align_path)\n            # One-hot encode each label and then use reduce_max to combine them into a single vector.\n            Y[i] = tf.reduce_max(tf.one_hot(labels, len(self.vocabulary)), axis=0)\n        \n        return X, Y\n\n# -------------------------------------------------------------------\n# Model Architecture\n# -------------------------------------------------------------------\ndef build_model(frame_length, image_height, image_width, vocabulary_size):\n    model = Sequential([\n        tf.keras.Input(shape=(frame_length, image_height, image_width, 1)),\n        # First 3D convolution block\n        Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        # Second 3D convolution block\n        Conv3D(128, kernel_size=(3, 3, 3), activation='relu'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        # Third 3D convolution block\n        Conv3D(256, kernel_size=(3, 3, 3), activation='relu'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        # Reshape to combine the spatial dimensions into a feature vector per time step.\n        Reshape((-1, 256)),\n        \n        # Temporal modeling with Bidirectional LSTMs\n        Bidirectional(LSTM(128, return_sequences=True)),\n        Dropout(0.5),\n        Bidirectional(LSTM(64)),\n        Dropout(0.5),\n        \n        # Dense layers for classification\n        Dense(256, activation='relu'),\n        Dropout(0.5),\n        Dense(vocabulary_size, activation='softmax')\n    ])\n    \n    return model\n\n# -------------------------------------------------------------------\n# Revised Training Routine\n# -------------------------------------------------------------------\ndef train_and_save_model(data_dir, alignment_dir, batch_size=16, epochs=30):\n    # -----------------------------------------------------------\n    # Step 1: Build the full vocabulary using all available data.\n    # -----------------------------------------------------------\n    full_data_generator = LipReadingDataGenerator(data_dir, alignment_dir, batch_size=batch_size)\n    full_vocabulary = full_data_generator.vocabulary\n    \n    # Retrieve all file paths (assumes they are in the same order)\n    all_video_paths = full_data_generator.video_paths\n    all_align_paths = full_data_generator.alignment_paths\n    \n    total_files = len(all_video_paths)\n    print(f\"Total number of files: {total_files}\")\n    \n    # -----------------------------------------------------------\n    # Step 2: Split the data into training and validation sets.\n    # Training: first half of the data.\n    # Validation: 70% of the second half.\n    # -----------------------------------------------------------\n    mid_point = total_files // 2\n    train_video_paths = all_video_paths[:mid_point]\n    train_align_paths = all_align_paths[:mid_point]\n    \n    second_half_video_paths = all_video_paths[mid_point:]\n    second_half_align_paths = all_align_paths[mid_point:]\n    \n    # Use 70% of the second half for validation.\n    val_count = int(len(second_half_video_paths) * 0.7)\n    val_video_paths = second_half_video_paths[:val_count]\n    val_align_paths = second_half_align_paths[:val_count]\n    \n    print(f\"Training on {len(train_video_paths)} files.\")\n    print(f\"Validating on {len(val_video_paths)} files.\")\n    \n    # -----------------------------------------------------------\n    # Step 3: Create data generators for training and validation.\n    # -----------------------------------------------------------\n    train_generator = LipReadingDataGenerator(\n        data_path=data_dir, \n        alignment_path=alignment_dir, \n        batch_size=batch_size,\n        video_paths=train_video_paths,\n        alignment_paths=train_align_paths,\n        fixed_vocabulary=full_vocabulary\n    )\n    \n    val_generator = LipReadingDataGenerator(\n        data_path=data_dir, \n        alignment_path=alignment_dir, \n        batch_size=batch_size,\n        video_paths=val_video_paths,\n        alignment_paths=val_align_paths,\n        fixed_vocabulary=full_vocabulary\n    )\n    \n    # -----------------------------------------------------------\n    # Step 4: Build, compile, and train the model.\n    # -----------------------------------------------------------\n    model = build_model(\n        frame_length=75,\n        image_height=46,\n        image_width=140,\n        vocabulary_size=len(full_vocabulary)\n    )\n    \n    model.compile(\n        optimizer=Adam(learning_rate=0.0001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    model_dir = \"models_main\"\n    os.makedirs(model_dir, exist_ok=True)\n    \n    # Callbacks to save the best model and to perform early stopping.\n    callbacks = [\n        ModelCheckpoint(\n            os.path.join(model_dir, 'lip_reading_best_model.keras'),\n            save_best_only=True,\n            monitor='val_accuracy',\n            mode='max'\n        ),\n        EarlyStopping(\n            monitor='val_loss',\n            patience=7,\n            restore_best_weights=True\n        )\n    ]\n    \n    print(\"Starting training...\")\n    history = model.fit(\n        train_generator,\n        epochs=epochs,\n        validation_data=val_generator,\n        callbacks=callbacks\n    )\n    \n    # -----------------------------------------------------------\n    # Step 5: Save the final model and the vocabulary.\n    # -----------------------------------------------------------\n    final_model_path = os.path.join(model_dir, 'lip_reading_full_model.h5')\n    model.save(final_model_path)\n    print(f\"Final model saved: {final_model_path}\")\n    \n    vocab_path = os.path.join(model_dir, 'vocabulary_main.txt')\n    with open(vocab_path, 'w') as f:\n        f.write('\\n'.join(full_vocabulary))\n    print(f\"Vocabulary saved: {vocab_path}\")\n    \n    return model, history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:44:39.033373Z","iopub.execute_input":"2025-02-19T08:44:39.033669Z","iopub.status.idle":"2025-02-19T08:44:39.057439Z","shell.execute_reply.started":"2025-02-19T08:44:39.033645Z","shell.execute_reply":"2025-02-19T08:44:39.056526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# -------------------------------------------------------------------\n# Example usage:\n# Set your directories for the video (.mpg) files and alignment (.align) files.\ndata_dir = r\"/kaggle/input/mouth-map-comp/data/s1\"\nalignment_dir = r\"/kaggle/input/mouth-map-comp/data/alignments/s1\"\ntrain_and_save_model(data_dir, alignment_dir, batch_size=16, epochs=30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:44:45.859710Z","iopub.execute_input":"2025-02-19T08:44:45.860002Z","iopub.status.idle":"2025-02-19T09:00:31.981864Z","shell.execute_reply.started":"2025-02-19T08:44:45.859979Z","shell.execute_reply":"2025-02-19T09:00:31.981091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_on_video(model_path, video_path, vocabulary_path, frame_length=75, \n                     image_height=46, image_width=140):\n    \"\"\"Make prediction on a single video file\"\"\"\n    # Load vocabulary\n    with open(vocabulary_path, 'r') as f:\n        vocabulary = f.read().strip().split('\\n')\n    \n    # Load model\n    model = load_model(model_path)\n    \n    # Process video\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        mouth = gray[190:236, 80:220]\n        mouth = cv2.resize(mouth, (image_width, image_height))\n        frames.append(mouth)\n    \n    cap.release()\n    \n    frames = np.array(frames, dtype=np.float32)\n    frames = (frames - frames.mean()) / (frames.std() + 1e-6)\n    \n    if len(frames) < frame_length:\n        pad_length = frame_length - len(frames)\n        frames = np.pad(frames, ((0, pad_length), (0, 0), (0, 0)), mode='constant')\n    else:\n        frames = frames[:frame_length]\n    \n    frames = frames.reshape(1, frame_length, image_height, image_width, 1)\n    \n    # Make prediction\n    prediction = model.predict(frames)[0]\n    top_indices = np.argsort(prediction)[-5:][::-1]  # Get top 5 predictions\n    \n    results = []\n    for idx in top_indices:\n        results.append({\n            'word': vocabulary[idx],\n            'confidence': float(prediction[idx])\n        })\n    \n    return results\n\nresults = predict_on_video(\n    model_path=\"/kaggle/working/models_main/lip_reading_best_model.keras\",\n    video_path=\"/kaggle/input/mouth-map-comp/data/s1/bbaf2n.mpg\",\n    vocabulary_path=\"/kaggle/working/models_main/vocabulary_main.txt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:07:51.527640Z","iopub.execute_input":"2025-02-19T09:07:51.527959Z","iopub.status.idle":"2025-02-19T09:07:52.974846Z","shell.execute_reply.started":"2025-02-19T09:07:51.527933Z","shell.execute_reply":"2025-02-19T09:07:52.974032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:08:02.976623Z","iopub.execute_input":"2025-02-19T09:08:02.976906Z","iopub.status.idle":"2025-02-19T09:08:02.982201Z","shell.execute_reply.started":"2025-02-19T09:08:02.976884Z","shell.execute_reply":"2025-02-19T09:08:02.981249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###################################################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T08:33:08.955261Z","iopub.execute_input":"2025-02-20T08:33:08.955538Z","iopub.status.idle":"2025-02-20T08:33:08.959134Z","shell.execute_reply.started":"2025-02-20T08:33:08.955515Z","shell.execute_reply":"2025-02-20T08:33:08.958224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, BatchNormalization, Reshape\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom glob import glob\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nclass LipReadingDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, data_path, alignment_path, batch_size=16, frame_length=75, \n                 image_height=46, image_width=140, video_paths=None, alignment_paths=None, \n                 fixed_vocabulary=None, augment=False, **kwargs):\n        super().__init__(**kwargs)\n        self.data_path = data_path\n        self.alignment_path = alignment_path\n        self.batch_size = batch_size\n        self.frame_length = frame_length\n        self.image_height = image_height\n        self.image_width = image_width\n        self.augment = augment\n\n        # Use provided paths or generate from directory\n        if video_paths is None:\n            self.video_paths = sorted(glob(os.path.join(data_path, '*.mpg')))\n        else:\n            self.video_paths = video_paths\n        \n        if alignment_paths is None:\n            self.alignment_paths = sorted(glob(os.path.join(alignment_path, '*.align')))\n        else:\n            self.alignment_paths = alignment_paths\n        \n        # Ensure alignment paths correspond to video paths\n        if len(self.video_paths) != len(self.alignment_paths):\n            # Match alignments to videos by base filename\n            video_basenames = [os.path.splitext(os.path.basename(v))[0] for v in self.video_paths]\n            alignment_dict = {os.path.splitext(os.path.basename(a))[0]: a for a in self.alignment_paths}\n            \n            # Reorder alignment paths to match video paths\n            self.alignment_paths = [alignment_dict.get(vb) for vb in video_basenames]\n            \n            # Filter out any None values (missing alignments)\n            valid_indices = [i for i, a in enumerate(self.alignment_paths) if a is not None]\n            self.video_paths = [self.video_paths[i] for i in valid_indices]\n            self.alignment_paths = [self.alignment_paths[i] for i in valid_indices]\n        \n        print(f\"Found {len(self.video_paths)} video files and {len(self.alignment_paths)} alignment files\")\n        \n        # Use fixed vocabulary if provided, otherwise create from data\n        if fixed_vocabulary is not None:\n            self.vocabulary = fixed_vocabulary\n        else:\n            self.vocabulary = self._create_word_vocabulary()\n            \n        self.char_to_num = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\")\n        self.num_to_char = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\", invert=True)\n\n    def _create_word_vocabulary(self):\n        words = set()\n        print(f\"Processing alignment files from: {self.alignment_path}\")\n        \n        for align_path in self.alignment_paths:\n            try:\n                with open(align_path, 'r') as f:\n                    content = f.read().strip().split()\n                    words.update([content[i] for i in range(2, len(content), 3)])\n            except Exception as e:\n                print(f\"Error processing {align_path}: {str(e)}\")\n        \n        words.discard('sil')\n        vocabulary = sorted(list(words))\n        \n        if not vocabulary:\n            print(\"No words found in alignment files. Using default vocabulary.\")\n            vocabulary = ['bin', 'blue', 'at', 'f', 'two', 'now']\n        \n        print(f\"Vocabulary size: {len(vocabulary)}\")\n        return vocabulary\n\n    def __len__(self):\n        return max(1, len(self.video_paths) // self.batch_size)\n    \n    def on_epoch_end(self):\n        \"\"\"Shuffle the dataset at the end of each epoch\"\"\"\n        indices = np.arange(len(self.video_paths))\n        np.random.shuffle(indices)\n        self.video_paths = [self.video_paths[i] for i in indices]\n        self.alignment_paths = [self.alignment_paths[i] for i in indices]\n    \n    def _apply_augmentation(self, frame):\n        \"\"\"Apply random augmentations to a frame\"\"\"\n        if self.augment and np.random.random() > 0.5:\n            # Random brightness adjustment\n            brightness = np.random.uniform(0.8, 1.2)\n            frame = frame * brightness\n            frame = np.clip(frame, 0, 255)\n            \n            # Random horizontal flip\n            if np.random.random() > 0.5:\n                frame = cv2.flip(frame, 1)\n                \n            # Random rotation (small angles)\n            if np.random.random() > 0.7:\n                angle = np.random.uniform(-5, 5)\n                h, w = frame.shape\n                center = (w/2, h/2)\n                M = cv2.getRotationMatrix2D(center, angle, 1.0)\n                frame = cv2.warpAffine(frame, M, (w, h))\n                \n        return frame\n    \n    def _process_video(self, video_path):\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            \n            # Extract mouth region - these coordinates might need adjustment\n            mouth = gray[190:236, 80:220]\n            \n            # Apply data augmentation if enabled\n            mouth = self._apply_augmentation(mouth)\n            \n            mouth = cv2.resize(mouth, (self.image_width, self.image_height))\n            frames.append(mouth)\n        \n        cap.release()\n        \n        frames = np.array(frames, dtype=np.float32)\n        \n        # Normalize frames - using per-video normalization\n        frames = (frames - frames.mean()) / (frames.std() + 1e-6)\n        \n        # Handle videos shorter or longer than desired frame length\n        if len(frames) < self.frame_length:\n            pad_length = self.frame_length - len(frames)\n            frames = np.pad(frames, ((0, pad_length), (0, 0), (0, 0)), mode='constant')\n        else:\n            frames = frames[:self.frame_length]\n        \n        return frames\n    \n    def _process_alignment(self, alignment_path):\n        with open(alignment_path, 'r') as f:\n            content = f.read().strip().split()\n        \n        words = [content[i] for i in range(2, len(content), 3) if content[i] != 'sil']\n        text = ' '.join(words)\n        return self.char_to_num(tf.convert_to_tensor(text.split()))\n    \n    def __getitem__(self, idx):\n        batch_videos = self.video_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_alignments = self.alignment_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        X = np.zeros((len(batch_videos), self.frame_length, self.image_height, self.image_width, 1))\n        Y = np.zeros((len(batch_videos), len(self.vocabulary)))\n        \n        for i, (video_path, align_path) in enumerate(zip(batch_videos, batch_alignments)):\n            frames = self._process_video(video_path)\n            X[i] = frames.reshape(self.frame_length, self.image_height, self.image_width, 1)\n            \n            labels = self._process_alignment(align_path)\n            Y[i] = tf.reduce_max(tf.one_hot(labels, len(self.vocabulary)), axis=0)\n        \n        return X, Y\n\ndef build_model(frame_length, image_height, image_width, vocabulary_size):\n    \"\"\"Build the lip reading model with improved architecture\"\"\"\n    model = Sequential([\n        tf.keras.Input(shape=(frame_length, image_height, image_width, 1)), \n        \n        # First 3D CNN block with increased filters\n        Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='same'),\n        Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='same'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        # Second 3D CNN block\n        Conv3D(128, kernel_size=(3, 3, 3), activation='relu', padding='same'),\n        Conv3D(128, kernel_size=(3, 3, 3), activation='relu', padding='same'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        # Third 3D CNN block\n        Conv3D(256, kernel_size=(3, 3, 3), activation='relu', padding='same'),\n        Conv3D(256, kernel_size=(3, 3, 3), activation='relu', padding='same'),\n        MaxPool3D(pool_size=(1, 2, 2)),\n        BatchNormalization(),\n        \n        # Reshape for sequence modeling\n        Reshape((-1, 256)),\n        \n        # Bidirectional LSTM layers\n        Bidirectional(LSTM(256, return_sequences=True, dropout=0.25, recurrent_dropout=0.1)),\n        Bidirectional(LSTM(128, return_sequences=False, dropout=0.25, recurrent_dropout=0.1)),\n        \n        # Dense layers with stronger regularization\n        Dense(512, activation='relu'),\n        Dropout(0.5),\n        Dense(vocabulary_size, activation='softmax')\n    ])\n    \n    return model\n\ndef train_with_validation_split(data_dir, alignment_dir, batch_size=16, epochs=50, learning_rate=0.0001):\n    \"\"\"Train the model using a proper train/validation split approach\"\"\"\n    print(\"Starting improved training with validation split...\")\n    \n    model_dir = \"models_improved\"\n    os.makedirs(model_dir, exist_ok=True)\n    \n    # Load all video and alignment paths\n    video_paths = sorted(glob(os.path.join(data_dir, '*.mpg')))\n    alignment_paths = sorted(glob(os.path.join(alignment_dir, '*.align')))\n    \n    # Match alignment paths with video paths by base filename\n    video_basenames = [os.path.splitext(os.path.basename(v))[0] for v in video_paths]\n    alignment_dict = {os.path.splitext(os.path.basename(a))[0]: a for a in alignment_paths}\n    matched_alignments = [alignment_dict.get(vb) for vb in video_basenames]\n    \n    # Filter out videos without matching alignments\n    valid_indices = [i for i, a in enumerate(matched_alignments) if a is not None]\n    filtered_videos = [video_paths[i] for i in valid_indices]\n    filtered_alignments = [matched_alignments[i] for i in valid_indices]\n    \n    print(f\"Total matched videos and alignments: {len(filtered_videos)}\")\n    \n    # Create a temporary generator to get consistent vocabulary across all splits\n    temp_generator = LipReadingDataGenerator(\n        data_path=data_dir,\n        alignment_path=alignment_dir,\n        batch_size=batch_size,\n        video_paths=filtered_videos,\n        alignment_paths=filtered_alignments\n    )\n    full_vocabulary = temp_generator.vocabulary\n    \n    # Split data: 50% training, 35% validation, 15% held out\n    # First split into training (50%) and remaining (50%)\n    train_videos, remaining_videos, train_alignments, remaining_alignments = train_test_split(\n        filtered_videos, filtered_alignments, test_size=0.5, random_state=42\n    )\n    \n    # Split the remaining data into validation (70% of remaining = 35% of total)\n    # and held out (30% of remaining = 15% of total)\n    val_videos, test_videos, val_alignments, test_alignments = train_test_split(\n        remaining_videos, remaining_alignments, test_size=0.3, random_state=42\n    )\n    \n    print(f\"Train set: {len(train_videos)} videos\")\n    print(f\"Validation set: {len(val_videos)} videos\")\n    print(f\"Test set (held out): {len(test_videos)} videos\")\n    \n    # Create generators for training and validation\n    train_generator = LipReadingDataGenerator(\n        data_path=data_dir,\n        alignment_path=alignment_dir,\n        batch_size=batch_size,\n        video_paths=train_videos,\n        alignment_paths=train_alignments,\n        fixed_vocabulary=full_vocabulary,\n        augment=True  # Enable data augmentation for training\n    )\n    \n    val_generator = LipReadingDataGenerator(\n        data_path=data_dir,\n        alignment_path=alignment_dir,\n        batch_size=batch_size,\n        video_paths=val_videos,\n        alignment_paths=val_alignments,\n        fixed_vocabulary=full_vocabulary,\n        augment=False  # No augmentation for validation\n    )\n    \n    # Build and compile model\n    model = build_model(\n        frame_length=75,\n        image_height=46,\n        image_width=140,\n        vocabulary_size=len(full_vocabulary)\n    )\n    \n    model.compile(\n        optimizer=Adam(learning_rate=learning_rate),\n        loss='categorical_crossentropy',\n        metrics=['accuracy', 'top_k_categorical_accuracy']  # Track top-k accuracy too\n    )\n    \n    model.summary()\n    \n    # Enhanced callbacks\n    callbacks = [\n        ModelCheckpoint(\n            os.path.join(model_dir, 'lip_reading_best_val_acc.keras'),\n            save_best_only=True,\n            monitor='val_accuracy',\n            mode='max',\n            verbose=1\n        ),\n        ModelCheckpoint(\n            os.path.join(model_dir, 'lip_reading_best_val_loss.keras'),\n            save_best_only=True,\n            monitor='val_loss',\n            mode='min',\n            verbose=1\n        ),\n        EarlyStopping(\n            monitor='val_loss',\n            patience=10,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-6,\n            verbose=1\n        )\n    ]\n    \n    # Train model with validation\n    history = model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=epochs,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Save final model\n    final_model_path = os.path.join(model_dir, 'lip_reading_final_model.keras')\n    model.save(final_model_path)\n    print(f\"Final model saved: {final_model_path}\")\n    \n    # Save vocabulary\n    vocab_path = os.path.join(model_dir, 'vocabulary.txt')\n    with open(vocab_path, 'w') as f:\n        f.write('\\n'.join(full_vocabulary))\n    print(f\"Vocabulary saved: {vocab_path}\")\n    \n    # Plot and save training history\n    plot_training_history(history, model_dir)\n    \n    # Evaluate on test set\n    test_generator = LipReadingDataGenerator(\n        data_path=data_dir,\n        alignment_path=alignment_dir,\n        batch_size=batch_size,\n        video_paths=test_videos,\n        alignment_paths=test_alignments,\n        fixed_vocabulary=full_vocabulary\n    )\n    \n    test_results = model.evaluate(test_generator)\n    print(f\"Test loss: {test_results[0]:.4f}\")\n    print(f\"Test accuracy: {test_results[1]:.4f}\")\n    print(f\"Test top-k accuracy: {test_results[2]:.4f}\")\n    \n    # Save test results\n    with open(os.path.join(model_dir, 'test_results.txt'), 'w') as f:\n        f.write(f\"Test loss: {test_results[0]:.4f}\\n\")\n        f.write(f\"Test accuracy: {test_results[1]:.4f}\\n\")\n        f.write(f\"Test top-k accuracy: {test_results[2]:.4f}\\n\")\n    \n    return model, history, full_vocabulary\n\ndef plot_training_history(history, save_dir):\n    \"\"\"Plot and save training history graphs\"\"\"\n    plt.figure(figsize=(12, 5))\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, 'training_history.png'))\n    plt.close()\n\ndef predict_on_video(model_path, video_path, vocabulary_path, frame_length=75, \n                     image_height=46, image_width=140):\n    \"\"\"Make prediction on a single video file\"\"\"\n    # Load vocabulary\n    with open(vocabulary_path, 'r') as f:\n        vocabulary = f.read().strip().split('\\n')\n    \n    # Load model\n    model = load_model(model_path)\n    \n    # Process video\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        mouth = gray[190:236, 80:220]\n        mouth = cv2.resize(mouth, (image_width, image_height))\n        frames.append(mouth)\n    \n    cap.release()\n    \n    frames = np.array(frames, dtype=np.float32)\n    frames = (frames - frames.mean()) / (frames.std() + 1e-6)\n    \n    if len(frames) < frame_length:\n        pad_length = frame_length - len(frames)\n        frames = np.pad(frames, ((0, pad_length), (0, 0), (0, 0)), mode='constant')\n    else:\n        frames = frames[:frame_length]\n    \n    frames = frames.reshape(1, frame_length, image_height, image_width, 1)\n    \n    # Make prediction\n    prediction = model.predict(frames)[0]\n    top_indices = np.argsort(prediction)[-5:][::-1]  # Get top 5 predictions\n    \n    results = []\n    for idx in top_indices:\n        results.append({\n            'word': vocabulary[idx],\n            'confidence': float(prediction[idx])\n        })\n    \n    return results\n\nif __name__ == \"__main__\":\n    # Example usage\n    data_dir = r\"/kaggle/input/mouth-map-comp/data/s1\"\n    alignment_dir = r\"/kaggle/input/mouth-map-comp/data/alignments/s1\"\n    \n    # Train model with validation split\n    model, history, vocabulary = train_with_validation_split(\n        data_dir=data_dir,\n        alignment_dir=alignment_dir,\n        batch_size=16,\n        epochs=50,\n        learning_rate=0.0001\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T08:36:01.130265Z","iopub.execute_input":"2025-02-20T08:36:01.130556Z","iopub.status.idle":"2025-02-20T08:37:04.548596Z","shell.execute_reply.started":"2025-02-20T08:36:01.130533Z","shell.execute_reply":"2025-02-20T08:37:04.547330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"####### ABOVE CODE GOT OOM... TRY CONDENSING DATA","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-21T03:36:33.286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, BatchNormalization, Reshape\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom glob import glob\nimport cv2\nimport argparse\n\nclass LipReadingDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, data_path, alignment_path, batch_size=16, frame_length=75, \n                 image_height=46, image_width=140, \n                 video_paths=None, alignment_paths=None, \n                 fixed_vocabulary=None):\n        super().__init__()\n        self.data_path = data_path\n        self.alignment_path = alignment_path\n        self.batch_size = batch_size\n        self.frame_length = frame_length\n        self.image_height = image_height\n        self.image_width = image_width\n\n        # Load video and alignment paths\n        if video_paths is None:\n            self.video_paths = sorted(glob(os.path.join(data_path, '*.mpg')))\n        else:\n            self.video_paths = video_paths\n        \n        if alignment_paths is None:\n            self.alignment_paths = sorted(glob(os.path.join(alignment_path, '*.align')))\n        else:\n            self.alignment_paths = alignment_paths\n        \n        # Vocabulary handling\n        if fixed_vocabulary is not None:\n            self.vocabulary = fixed_vocabulary\n        else:\n            self.vocabulary = self._create_word_vocabulary()\n        \n        self.char_to_num = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\")\n        self.num_to_char = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary, oov_token=\"\", invert=True)\n\n    def _create_word_vocabulary(self):\n        words = set()\n        for align_path in self.alignment_paths:\n            with open(align_path, 'r') as f:\n                content = f.read().strip().split()\n                words.update(content[2::3])\n        words.discard('sil')\n        return sorted(list(words)) if words else ['default_word']\n\n    def __len__(self):\n        return max(1, len(self.video_paths) // self.batch_size)\n\n    def _process_video(self, video_path):\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            mouth = gray[190:236, 80:220]\n            mouth = cv2.resize(mouth, (self.image_width, self.image_height))\n            frames.append(mouth)\n        cap.release()\n\n        # Normalize and pad frames\n        frames = np.array(frames, dtype=np.float32)\n        if frames.shape[0] < self.frame_length:\n            frames = pad_sequences([frames], maxlen=self.frame_length, dtype='float32', padding='post')[0]\n        else:\n            frames = frames[:self.frame_length]\n        frames = (frames - frames.mean()) / (frames.std() + 1e-7)\n        return frames\n\n    def _process_alignment(self, alignment_path):\n        with open(alignment_path, 'r') as f:\n            content = f.read().strip().split()\n        words = [word for i, word in enumerate(content[2::3]) if content[i] != 'sil']\n        text = ' '.join(words)\n        labels = self.char_to_num(tf.convert_to_tensor(text.split()))\n        return tf.reduce_max(tf.one_hot(labels, len(self.vocabulary)), axis=0)\n\n    def __getitem__(self, idx):\n        batch_videos = self.video_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_alignments = self.alignment_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        X = np.zeros((len(batch_videos), self.frame_length, self.image_height, self.image_width, 1))\n        Y = []\n        \n        for i, (video_path, align_path) in enumerate(zip(batch_videos, batch_alignments)):\n            frames = self._process_video(video_path)\n            X[i] = frames.reshape(self.frame_length, self.image_height, self.image_width, 1)\n            Y.append(self._process_alignment(align_path))\n        \n        Y = np.array(Y)\n        return X, Y\n\ndef build_model(frame_length, image_height, image_width, vocabulary_size):\n    model = Sequential([\n        tf.keras.Input(shape=(frame_length, image_height, image_width, 1)), \n        Conv3D(64, (3, 3, 3), activation='relu'),\n        MaxPool3D((1, 2, 2)),\n        BatchNormalization(),\n        Conv3D(128, (3, 3, 3), activation='relu'),\n        MaxPool3D((1, 2, 2)),\n        BatchNormalization(),\n        Conv3D(256, (3, 3, 3), activation='relu'),\n        MaxPool3D((1, 2, 2)),\n        BatchNormalization(),\n        Reshape((-1, 256)),\n        Bidirectional(LSTM(128, return_sequences=True)),\n        Dropout(0.5),\n        Bidirectional(LSTM(64)),\n        Dropout(0.5),\n        Dense(256, activation='relu'),\n        Dropout(0.5),\n        Dense(vocabulary_size, activation='sigmoid')\n    ])\n    return model\n\ndef train_and_save_main_model(data_dir, alignment_dir, batch_size=16):\n    model_dir = \"models_main\"\n    os.makedirs(model_dir, exist_ok=True)\n    \n    full_data_generator = LipReadingDataGenerator(data_dir, alignment_dir, batch_size=batch_size)\n    full_vocabulary = full_data_generator.vocabulary\n    total_videos = len(full_data_generator.video_paths)\n    train_size = total_videos // 2\n    remaining = total_videos - train_size\n    val_size = int(0.7 * remaining)\n    test_size = remaining - val_size\n\n    # Split video and alignment paths\n    video_paths = full_data_generator.video_paths\n    alignment_paths = full_data_generator.alignment_paths\n\n    train_videos = video_paths[:train_size]\n    train_alignments = alignment_paths[:train_size]\n\n    val_videos = video_paths[train_size:train_size + val_size]\n    val_alignments = alignment_paths[train_size:train_size + val_size]\n\n    test_videos = video_paths[train_size + val_size:]\n    test_alignments = alignment_paths[train_size + val_size:]\n\n    # Create data generators\n    train_generator = LipReadingDataGenerator(\n        video_paths=train_videos,\n        alignment_paths=train_alignments,\n        data_path=data_dir,\n        alignment_path=alignment_dir,\n        batch_size=batch_size,\n        fixed_vocabulary=full_vocabulary\n    )\n\n    val_generator = LipReadingDataGenerator(\n        video_paths=val_videos,\n        alignment_paths=val_alignments,\n        data_path=data_dir,\n        alignment_path=alignment_dir,\n        batch_size=batch_size,\n        fixed_vocabulary=full_vocabulary\n    )\n\n    test_generator = LipReadingDataGenerator(\n        video_paths=test_videos,\n        alignment_paths=test_alignments,\n        data_path=data_dir,\n        alignment_path=alignment_dir,\n        batch_size=batch_size,\n        fixed_vocabulary=full_vocabulary\n    )\n\n    # Build and compile the model\n    model = build_model(\n        frame_length=75,\n        image_height=46,\n        image_width=140,\n        vocabulary_size=len(full_vocabulary)\n    )\n\n    model.compile(\n        optimizer=Adam(learning_rate=0.0001),\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    checkpoint = ModelCheckpoint(\n        os.path.join(model_dir, 'best_lip_reading_model.keras'),\n        save_best_only=True,\n        monitor='val_accuracy',\n        mode='max'\n    )\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=7,\n        restore_best_weights=True\n    )\n\n    # Train the model\n    history = model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=10,\n        callbacks=[checkpoint, early_stopping]\n    )\n\n    # Evaluate on the test set\n    best_model = tf.keras.models.load_model(os.path.join(model_dir, 'best_lip_reading_model.keras'))\n    test_loss, test_acc = best_model.evaluate(test_generator)\n    print(f\"Test accuracy: {test_acc}\")\n\n    # Save the final model\n    final_model_path = os.path.join(model_dir, 'lip_reading_final_model.keras')\n    best_model.save(final_model_path)\n    print(f\"Final model saved: {final_model_path}\")\n\n    # Save the vocabulary\n    vocab_path = os.path.join(model_dir, 'vocabulary_main.txt')\n    with open(vocab_path, 'w') as f:\n        f.write('\\n'.join(full_vocabulary))\n    print(f\"Vocabulary saved: {vocab_path}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='Lip Reading Model Training')\n    parser.add_argument('--data_dir', type=str, required=True, help='Path to video data directory')\n    parser.add_argument('--alignment_dir', type=str, required=True, help='Path to alignment data directory')\n    parser.add_argument('--batch_size', type=int, default=16, help='Batch size for training')\n    args = parser.parse_args()\n\n    # Validate directories\n    if not os.path.exists(args.data_dir):\n        raise ValueError(f\"Data directory {args.data_dir} does not exist\")\n    if not os.path.exists(args.alignment_dir):\n        raise ValueError(f\"Alignment directory {args.alignment_dir} does not exist\")\n\n    # Start training\n    train_and_save_main_model(args.data_dir, args.alignment_dir, args.batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:59:14.693701Z","iopub.execute_input":"2025-02-21T12:59:14.693996Z","iopub.status.idle":"2025-02-21T12:59:22.724637Z","shell.execute_reply.started":"2025-02-21T12:59:14.693977Z","shell.execute_reply":"2025-02-21T12:59:22.723701Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data_dir = r\"/kaggle/input/mouth-map-comp/data/s1\"\nalignment_dir = r\"/kaggle/input/mouth-map-comp/data/alignments/s1\"\nbatch_size = 16\ntrain_and_save_main_model(data_dir, alignment_dir, batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:59:49.647666Z","iopub.execute_input":"2025-02-21T12:59:49.648265Z","iopub.status.idle":"2025-02-21T13:13:33.430429Z","shell.execute_reply.started":"2025-02-21T12:59:49.648217Z","shell.execute_reply":"2025-02-21T13:13:33.429527Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 3s/step - accuracy: 0.0397 - loss: 0.6703 - val_accuracy: 0.0179 - val_loss: 0.6675\nEpoch 2/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.2138 - loss: 0.5600 - val_accuracy: 1.0000 - val_loss: 0.5693\nEpoch 3/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.5487 - loss: 0.4476 - val_accuracy: 1.0000 - val_loss: 0.4557\nEpoch 4/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.7567 - loss: 0.3754 - val_accuracy: 1.0000 - val_loss: 0.3838\nEpoch 5/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.8875 - loss: 0.3467 - val_accuracy: 1.0000 - val_loss: 0.3557\nEpoch 6/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 2s/step - accuracy: 0.9308 - loss: 0.3258 - val_accuracy: 1.0000 - val_loss: 0.3411\nEpoch 7/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - accuracy: 0.9427 - loss: 0.3171 - val_accuracy: 1.0000 - val_loss: 0.3268\nEpoch 8/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 2s/step - accuracy: 0.9822 - loss: 0.3123 - val_accuracy: 1.0000 - val_loss: 0.3210\nEpoch 9/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.9438 - loss: 0.3034 - val_accuracy: 1.0000 - val_loss: 0.3206\nEpoch 10/10\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.9849 - loss: 0.3023 - val_accuracy: 1.0000 - val_loss: 0.3216\n\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.5672\nTest accuracy: 1.0\nFinal model saved: models_main/lip_reading_final_model.keras\nVocabulary saved: models_main/vocabulary_main.txt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}